{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ42G3CiV0Nc"
      },
      "source": [
        "# Import data and setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7Pmq_ZBfbpN"
      },
      "outputs": [],
      "source": [
        "! pip install -q transformers\n",
        "! pip install -q textacy\n",
        "! pip install -q PyDrive\n",
        "! pip install -q wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brr96lYkbRhX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import gensim\n",
        "import string\n",
        "import re\n",
        "import unicodedata\n",
        "import textacy\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk.stem import PorterStemmer\n",
        "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification, TextClassificationPipeline\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import zipfile\n",
        "\n",
        "from google.colab import auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "rs = 27 # random_state - when/if needed\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_df_to_gDrive(df:pd.DataFrame, file_name:str, folder_id:str, idx:bool=False):\n",
        "    csv_file = df.to_csv(file_name, index=idx)\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false and title='{}'\".format(folder_id, file_name)}).GetList()\n",
        "    if len(file_list) > 0:\n",
        "        f = file_list[0]\n",
        "    else:\n",
        "        f = drive.CreateFile({\n",
        "            'title': file_name,\n",
        "            'parents': [{'kind': 'drive#fileLink', 'id': folder_id}],\n",
        "            'overwrite': True\n",
        "        })\n",
        "    with open(file_name, 'r') as temp:\n",
        "        fc = temp.read()\n",
        "    f.SetContentString(fc)\n",
        "    f.Upload()\n",
        "\n",
        "def my_gDrive_to_colab(file_name:str, folder_id:str):\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false and title='{}'\".format(folder_id, file_name)}).GetList()\n",
        "    if len(file_list) > 0:\n",
        "        f = file_list[0]\n",
        "        f.GetContentFile(file_name)\n",
        "        return True\n",
        "    else:\n",
        "        print(f'File {file_name} does not exists in this Drive')\n",
        "        return False\n",
        "\n",
        "def my_register_scores(df_from:pd.DataFrame, df_to:pd.DataFrame, method_name:str):\n",
        "    y_true = df_from['y_true']\n",
        "    y_pred = df_from['y_pred']\n",
        "\n",
        "    df_to.loc[method_name, 'accuracy_score'] = accuracy_score(y_true, y_pred)\n",
        "    df_to.loc[method_name, 'precision_score'] = precision_score(y_true, y_pred)\n",
        "    df_to.loc[method_name, 'recall_score'] = recall_score(y_true, y_pred)\n",
        "    df_to.loc[method_name, 'f1_score'] = f1_score(y_true, y_pred)\n",
        "    # df_to.loc[method_name, 'cm'] = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "def my_preprocess_string(\n",
        "    x:str,\n",
        "    all_lower:bool=True,\n",
        "    remove_numbers:bool=True,\n",
        "    replace_accents:bool=True,\n",
        "    # fix_spelling:bool=True,\n",
        "    # expand_abbreviations:bool=True,\n",
        "    handle_negation:bool=True,\n",
        "    remove_ponctuation:bool=True,\n",
        "    remove_stop_words:bool=True,\n",
        "    apply_stemming:bool=True,\n",
        "    remove_white_spaces:bool=True\n",
        "):\n",
        "    if all_lower:\n",
        "        x = x.lower()\n",
        "    if remove_numbers:\n",
        "        x = re.sub(r'\\d+', '', x)\n",
        "    if replace_accents:\n",
        "        x = ''.join(c for c in unicodedata.normalize('NFD', x) if unicodedata.category(c) != 'Mn')\n",
        "    if remove_white_spaces:\n",
        "        x = re.sub(' +', ' ', x).strip()\n",
        "    # if fix_spelling:\n",
        "    #     pass\n",
        "    # if expand_abbreviations:\n",
        "    #     pass\n",
        "    if handle_negation:\n",
        "        x = ' '.join(mark_negation(word_tokenize(re.sub(r\"[^\\w\\s']\", '.', x))))\n",
        "    if remove_ponctuation:\n",
        "        x = re.sub(r\"[^\\w\\s]\", ' ', x)\n",
        "    if remove_stop_words:\n",
        "        x = ' '.join(xi for xi in x.split() if (xi if not xi.endswith('_NEG') else xi[:-4]) not in set(stopwords.words('english')))\n",
        "    if apply_stemming:\n",
        "        x = ' '.join(stemmer.stem(xi if not xi.endswith('_NEG') else xi[:-4]) + ('' if not xi.endswith('_NEG') else '_NEG') for xi in x.split())\n",
        "    if remove_white_spaces:\n",
        "        x = re.sub(' +', ' ', x).strip()\n",
        "    \n",
        "    return x\n",
        "\n",
        "def my_preprocessor(\n",
        "    df:pd.DataFrame,\n",
        "    all_lower:bool=True,\n",
        "    remove_numbers:bool=True,\n",
        "    replace_accents:bool=True,\n",
        "    # fix_spelling:bool=True,\n",
        "    # expand_abbreviations:bool=True,\n",
        "    handle_negation:bool=True,\n",
        "    remove_ponctuation:bool=True,\n",
        "    remove_stop_words:bool=True,\n",
        "    apply_stemming:bool=True,\n",
        "    remove_white_spaces:bool=True\n",
        "):\n",
        "    dff = df.copy()\n",
        "    dff['text'] = dff['text'].map(lambda x: my_preprocess_string(\n",
        "        x,\n",
        "        all_lower=all_lower,\n",
        "        remove_numbers=remove_numbers,\n",
        "        replace_accents=replace_accents,\n",
        "        # fix_spelling=fix_spelling,\n",
        "        # expand_abbreviations=expand_abbreviations,\n",
        "        handle_negation=handle_negation,\n",
        "        remove_ponctuation=remove_ponctuation,\n",
        "        remove_stop_words=remove_stop_words,\n",
        "        apply_stemming=apply_stemming,\n",
        "        remove_white_spaces=remove_white_spaces\n",
        "    ))\n",
        "\n",
        "    return dff\n",
        "\n",
        "def my_explode_text(df:pd.DataFrame):\n",
        "    dff = df.copy()    \n",
        "\n",
        "    dff['word'] = dff['text'].str.split()\n",
        "    dff['original_index'] = dff.index\n",
        "    dff = dff.explode('word')\n",
        "\n",
        "    return dff\n",
        "\n",
        "def my_invert_sl(df):\n",
        "    df_neg = df.copy()\n",
        "    df_neg['English'] += '_NEG'\n",
        "    df_neg['Positive'] = 1 - df_neg['Positive']\n",
        "    df_neg['Negative'] = 1 - df_neg['Negative']\n",
        "    \n",
        "    return pd.concat([df, df_neg])\n",
        "\n",
        "def my_bag_of_words(df_train, df_test, model):\n",
        "    vectorizer = eval(model)\n",
        "    X_BOG_train = vectorizer.fit_transform(df_train['text'])\n",
        "    X_BOG_test = vectorizer.transform(df_test['text'])\n",
        "\n",
        "    return (X_BOG_train, X_BOG_test, vectorizer)\n",
        "\n",
        "def my_str2vec(df_train, df_test, vs, w, mc):\n",
        "    dff_train = df_train.copy()\n",
        "    \n",
        "\n",
        "    tokenized_sentences_train = [sentence.split() for sentence in dff_train['text']]\n",
        "    train_words = set()\n",
        "    for sentence in tokenized_sentences_train:\n",
        "        for word in sentence:\n",
        "            train_words.add(str(word))\n",
        "    try:\n",
        "        model = gensim.models.Word2Vec(tokenized_sentences_train, size=vs, window=w, min_count=mc, workers=-1)\n",
        "        s2v_train = np.array([\n",
        "            np.mean([model[word] for word in sentence], axis=0)\n",
        "            for sentence in tokenized_sentences_train\n",
        "        ])\n",
        "        dff_test = df_test.copy()\n",
        "        tokenized_sentences_test = [sentence.split() for sentence in dff_test['text']]\n",
        "        s2v_test = np.array([\n",
        "            np.mean([model[word] if str(word) in train_words else np.zeros(model.vector_size) for word in sentence], axis=0)\n",
        "            for sentence in tokenized_sentences_test\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        model = gensim.models.Word2Vec(tokenized_sentences_train, vector_size=vs, window=w, min_count=mc, workers=-1)\n",
        "        s2v_train = np.array([\n",
        "            np.mean([model.wv[word] for word in sentence], axis=0)\n",
        "            for sentence in tokenized_sentences_train\n",
        "        ])\n",
        "        dff_test = df_test.copy()\n",
        "        tokenized_sentences_test = [sentence.split() for sentence in dff_test['text']]\n",
        "        s2v_test = np.array([\n",
        "            np.mean([model.wv[word] if str(word) in train_words else np.zeros(model.vector_size) for word in sentence], axis=0)\n",
        "            for sentence in tokenized_sentences_test\n",
        "        ])\n",
        "    \n",
        "    return (\n",
        "        s2v_train,\n",
        "        s2v_test,\n",
        "        model\n",
        "    )\n",
        "\n",
        "def my_baseline(df:pd.DataFrame):\n",
        "    dff = df.copy()\n",
        "    dff['y'] = dff['text'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
        "    dff['y_true'] = np.where(dff['label'] == 'pos', 1, 0)\n",
        "    dff['y_pred'] = np.where(dff['y'] >= 0, 1, 0)\n",
        "    return dff\n",
        "\n",
        "def my_sentiment_lexicon(df:pd.DataFrame, df_sl:pd.DataFrame):\n",
        "    dff = df.copy()\n",
        "    dff_sl = my_explode_text(df). \\\n",
        "        merge(df_sl, left_on='word', right_on='English'). \\\n",
        "        groupby('original_index'). \\\n",
        "        agg({'Positive': 'sum', 'Negative': 'sum'})\n",
        "    dff_sl['pred_label'] = np.where(dff_sl['Positive'] >= dff_sl['Negative'], 'pos', 'neg')\n",
        "\n",
        "    dff['pred_label'] = dff_sl['pred_label']\n",
        "    dff['y_pred'] = np.where(dff['pred_label'] == 'pos', 1, 0)\n",
        "    dff['y_true'] = np.where(dff['label'] == 'pos', 1, 0)\n",
        "    \n",
        "    return dff\n",
        "\n",
        "def my_ML(X_key, embeddings_key, models_key, X, embeddings, models):\n",
        "    if embeddings_key[:3]=='BOW':\n",
        "        X_train, X_test, vectorizer = my_bag_of_words(*X[X_key], embeddings[embeddings_key])\n",
        "    else:\n",
        "        X_train, X_test, vectorizer = my_str2vec(*X[X_key], *embeddings[embeddings_key])\n",
        "\n",
        "    clf = eval(models[models_key])\n",
        "    model = clf.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    my_register_scores(\n",
        "        pd.DataFrame({'y_true': y_train, 'y_pred': y_train_pred}),\n",
        "        df_results,\n",
        "        f'ML - {X_key} - {embeddings_key} - {models_key} - train'\n",
        "    )\n",
        "    my_register_scores(\n",
        "        pd.DataFrame({'y_true': y_test, 'y_pred': y_test_pred}),\n",
        "        df_results,\n",
        "        f'ML - {X_key} - {embeddings_key} - {models_key} - test'\n",
        "    )\n",
        "    return model, vectorizer\n",
        "\n",
        "def my_predict(string_to_sa, prep_version, vec, clf):\n",
        "    str_to_sa = my_preprocess_string(string_to_sa, **prep_version)\n",
        "    vec_to_sa = vec.transform([str_to_sa])\n",
        "    return clf.predict_proba(vec_to_sa)[0][1], str_to_sa, vec_to_sa\n",
        "\n",
        "def my_sa(string_to_sa, prep_version, vec, clf):\n",
        "    p, s, v = my_predict(string_to_sa, prep_version, vec, clf)\n",
        "    df_coef = pd.DataFrame({\n",
        "        'val': pd.Series(np.array(v.todense())[0]) * pd.Series(clf.coef_[0]),\n",
        "        'name': vec.get_feature_names_out()\n",
        "    })\n",
        "    return p, s, df_coef[df_coef['val'] != 0]\n",
        "\n",
        "def my_wc(v):\n",
        "    from wordcloud import WordCloud\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color='white',\n",
        "        color_func=lambda *args, **kwargs: dict(zip(v['name'], np.where(v['val']>0, '#00FF00', '#FF0000'))).get(args[0])\n",
        "    ).generate_from_frequencies(dict(zip(v['name'], v['val'].abs())))\n",
        "                        \n",
        "    plt.figure(figsize = (8, 8), facecolor = None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad = 0)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ghgv2xvEWbvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYI26QndMuR6"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "folder_id = '1-UkmMimz0POOr4LXOv1XDVrc8Ktnxk2k'\n",
        "download_list = ['imdb_reviews_test.csv', 'imdb_reviews_train.csv', 'NCR-lexicon.csv']\n",
        "\n",
        "for i in download_list:\n",
        "    my_gDrive_to_colab(i, folder_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo_FFyLzWBql"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('imdb_reviews_test.csv')\n",
        "df_train = pd.read_csv('imdb_reviews_train.csv')\n",
        "\n",
        "df_sl_raw = pd.read_csv('NCR-lexicon.csv')\n",
        "df_sl_raw = df_sl_raw[(df_sl_raw['Positive']==1) | (df_sl_raw['Negative']==1)].reset_index(drop=True)\n",
        "\n",
        "df_sl = my_invert_sl(df_sl_raw)\n",
        "\n",
        "df_results = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_train), '\\n')\n",
        "print(len(df_test), '\\n')\n",
        "print(df_train['text'].map(lambda x: len(x)).describe(), '\\n')\n",
        "print(df_test['text'].map(lambda x: len(x)).describe(), '\\n')\n",
        "print(df_train['label'].map(lambda x: 1 if x == 'pos' else 0).describe(), '\\n')\n",
        "print(df_test['label'].map(lambda x: 1 if x == 'pos' else 0).describe(), '\\n')"
      ],
      "metadata": {
        "id": "B6XFCcePSoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTyNSPCUaAgA"
      },
      "source": [
        "# Pre-processed Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoXGeNxnZ_2_"
      },
      "outputs": [],
      "source": [
        "prep_v1 = {\n",
        "    'all_lower': True,\n",
        "    'remove_numbers': True,\n",
        "    'replace_accents': True,\n",
        "    # 'fix_spelling': True,\n",
        "    # 'expand_abbreviations': True,\n",
        "    'handle_negation': True,\n",
        "    'remove_ponctuation': True,\n",
        "    'remove_stop_words': True,\n",
        "    'apply_stemming': True,\n",
        "    'remove_white_spaces': True\n",
        "}\n",
        "\n",
        "prep_v2 = {\n",
        "    'all_lower': True,\n",
        "    'remove_numbers': True,\n",
        "    'replace_accents': True,\n",
        "    # 'fix_spelling': True,\n",
        "    # 'expand_abbreviations': True,\n",
        "    'handle_negation': True,\n",
        "    'remove_ponctuation': True,\n",
        "    'remove_stop_words': False,\n",
        "    'apply_stemming': False,\n",
        "    'remove_white_spaces': True\n",
        "}\n",
        "\n",
        "prep_v3 = {\n",
        "    'all_lower': True,\n",
        "    'remove_numbers': True,\n",
        "    'replace_accents': True,\n",
        "    # 'fix_spelling': True,\n",
        "    # 'expand_abbreviations': True,\n",
        "    'handle_negation': True,\n",
        "    'remove_ponctuation': True,\n",
        "    'remove_stop_words': True,\n",
        "    'apply_stemming': False,\n",
        "    'remove_white_spaces': True\n",
        "}\n",
        "\n",
        "prep_v4 = {\n",
        "    'all_lower': True,\n",
        "    'remove_numbers': True,\n",
        "    'replace_accents': True,\n",
        "    # 'fix_spelling': True,\n",
        "    # 'expand_abbreviations': True,\n",
        "    'handle_negation': False,\n",
        "    'remove_ponctuation': True,\n",
        "    'remove_stop_words': True,\n",
        "    'apply_stemming': False,\n",
        "    'remove_white_spaces': True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYLTT2U0bNSj"
      },
      "outputs": [],
      "source": [
        "if my_gDrive_to_colab('df_test_v1.csv', folder_id):\n",
        "    df_test_v1 = pd.read_csv('df_test_v1.csv')\n",
        "else:\n",
        "    df_test_v1 = my_preprocessor(df_test, **prep_v1)\n",
        "    my_df_to_gDrive(df_test_v1, 'df_test_v1.csv', folder_id)\n",
        "    \n",
        "if my_gDrive_to_colab('df_train_v1.csv', folder_id):\n",
        "    df_train_v1 = pd.read_csv('df_train_v1.csv')\n",
        "else:\n",
        "    df_train_v1 = my_preprocessor(df_train, **prep_v1)\n",
        "    my_df_to_gDrive(df_train_v1, 'df_train_v1.csv', folder_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfVIIc5IbPum"
      },
      "outputs": [],
      "source": [
        "if my_gDrive_to_colab('df_test_v2.csv', folder_id):\n",
        "    df_test_v2 = pd.read_csv('df_test_v2.csv')\n",
        "else:\n",
        "    df_test_v2 = my_preprocessor(df_test, **prep_v2)\n",
        "    my_df_to_gDrive(df_test_v2, 'df_test_v2.csv', folder_id)\n",
        "    \n",
        "if my_gDrive_to_colab('df_train_v2.csv', folder_id):\n",
        "    df_train_v2 = pd.read_csv('df_train_v2.csv')\n",
        "else:\n",
        "    df_train_v2 = my_preprocessor(df_train, **prep_v2)\n",
        "    my_df_to_gDrive(df_train_v2, 'df_train_v2.csv', folder_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bOe8Ll7cwQR"
      },
      "outputs": [],
      "source": [
        "if my_gDrive_to_colab('df_test_v3.csv', folder_id):\n",
        "    df_test_v3 = pd.read_csv('df_test_v3.csv')\n",
        "else:\n",
        "    df_test_v3 = my_preprocessor(df_test, **prep_v3)\n",
        "    my_df_to_gDrive(df_test_v3, 'df_test_v3.csv', folder_id)\n",
        "    \n",
        "if my_gDrive_to_colab('df_train_v3.csv', folder_id):\n",
        "    df_train_v3 = pd.read_csv('df_train_v3.csv')\n",
        "else:\n",
        "    df_train_v3 = my_preprocessor(df_train, **prep_v3)\n",
        "    my_df_to_gDrive(df_train_v3, 'df_train_v3.csv', folder_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if my_gDrive_to_colab('df_test_v4.csv', folder_id):\n",
        "    df_test_v4 = pd.read_csv('df_test_v4.csv')\n",
        "else:\n",
        "    df_test_v4 = my_preprocessor(df_test, **prep_v4)\n",
        "    my_df_to_gDrive(df_test_v4, 'df_test_v4.csv', folder_id)\n",
        "    \n",
        "if my_gDrive_to_colab('df_train_v4.csv', folder_id):\n",
        "    df_train_v4 = pd.read_csv('df_train_v4.csv')\n",
        "else:\n",
        "    df_train_v4 = my_preprocessor(df_train, **prep_v3)\n",
        "    my_df_to_gDrive(df_train_v4, 'df_train_v4.csv', folder_id)"
      ],
      "metadata": {
        "id": "RzbMvvNhz7Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPJBgZ2wWB8h"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgbL--IwWF55"
      },
      "outputs": [],
      "source": [
        "df_baseline = my_baseline(df_test)\n",
        "my_register_scores(df_baseline, df_results, 'TextBlob')\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lav0x3B6WGIi"
      },
      "source": [
        "# Using Sentiment Lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdtWINtcWTTk"
      },
      "outputs": [],
      "source": [
        "df_sentiment_lexicon_raw = my_sentiment_lexicon(df_test, df_sl)\n",
        "my_register_scores(df_sentiment_lexicon_raw, df_results, 'Sentiment Lexicon - Raw')\n",
        "\n",
        "df_sentiment_lexicon_v1 = my_sentiment_lexicon(df_test_v1, df_sl)\n",
        "my_register_scores(df_sentiment_lexicon_v1, df_results, 'Sentiment Lexicon - v1_raw')\n",
        "\n",
        "df_sl_v1 = my_invert_sl(\n",
        "    my_preprocessor(df_sl_raw.rename({'English': 'text'}, axis='columns'), **prep_v1). \\\n",
        "    rename({'text': 'English'}, axis='columns'). \\\n",
        "    groupby('English', as_index=False). \\\n",
        "    agg({'Positive': 'max', 'Negative': 'max'})\n",
        ")\n",
        "\n",
        "df_sentiment_lexicon_v1_1 = my_sentiment_lexicon(df_test_v1, df_sl_v1)\n",
        "my_register_scores(df_sentiment_lexicon_v1_1, df_results, 'Sentiment Lexicon - v1_1')\n",
        "\n",
        "df_sentiment_lexicon_v2_2 = my_sentiment_lexicon(df_test_v2, df_sl)\n",
        "my_register_scores(df_sentiment_lexicon_v2_2, df_results, 'Sentiment Lexicon - v2_raw')\n",
        "\n",
        "df_sl_v2 = my_invert_sl(\n",
        "    my_preprocessor(\n",
        "        df_sl_raw.rename({'English': 'text'}, axis='columns'),\n",
        "        **prep_v2\n",
        "    ). \\\n",
        "    rename({'text': 'English'}, axis='columns'). \\\n",
        "    groupby('English', as_index=False). \\\n",
        "    agg({'Positive': 'max', 'Negative': 'max'})\n",
        ")\n",
        "\n",
        "df_sentiment_lexicon_v2_2 = my_sentiment_lexicon(df_test_v2, df_sl_v2)\n",
        "my_register_scores(df_sentiment_lexicon_v2_2, df_results, 'Sentiment Lexicon - v2_2')\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTF7HqT5WT0o"
      },
      "source": [
        "# Using Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhlMbsfRdAb0"
      },
      "outputs": [],
      "source": [
        "y_train = np.where(df_train['label']=='pos', 1, 0)\n",
        "y_test = np.where(df_test['label']=='pos', 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR6pPqITggyb"
      },
      "outputs": [],
      "source": [
        "X = {\n",
        "    'raw': (df_train, df_test),\n",
        "    'v1': (df_train_v1, df_test_v1),\n",
        "    'v2': (df_train_v2, df_test_v2),\n",
        "    'v3': (df_train_v3, df_test_v3),\n",
        "    'v4': (df_train_v4, df_test_v4),\n",
        "}\n",
        "\n",
        "embeddings = {\n",
        "    'BOW_default': \"TfidfVectorizer()\",\n",
        "    'BOW_sa_sw': \"TfidfVectorizer(strip_accents='ascii', stop_words='english')\",\n",
        "    's2v_100_5_1': (100, 5, 1),\n",
        "    's2v_200_5_1': (200, 5, 1),\n",
        "}\n",
        "\n",
        "models = {\n",
        "    'bnb': \"BernoulliNB()\",\n",
        "    'lr': \"LogisticRegression(max_iter=1000, random_state=rs)\",\n",
        "    'svm': \"svm.LinearSVC(random_state=rs)\",\n",
        "    'dtc_10': \"DecisionTreeClassifier(max_depth=10, random_state=rs)\",\n",
        "    'rfc_10': \"RandomForestClassifier(max_depth=10, random_state=rs)\",\n",
        "    'dtc_20': \"DecisionTreeClassifier(max_depth=20, random_state=rs)\",\n",
        "    'rfc_20': \"RandomForestClassifier(max_depth=20, random_state=rs)\",\n",
        "}\n",
        "\n",
        "save_to = 'drive/MyDrive/ML_models/'\n",
        "\n",
        "for i in X:\n",
        "    for j in embeddings:\n",
        "        for k in models:\n",
        "            m_v = my_ML(i, j, k, X, embeddings, models)\n",
        "            with open(f'{save_to}{i}-{j}-{k}.pickle', 'wb') as f:\n",
        "                pickle.dump(m_v, f)\n",
        "\n",
        "df_results.loc[df_results.index.str.endswith('- test')].sort_values('accuracy_score', ascending=False).iloc[:30]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_ft = {\n",
        "    'v2': (df_train_v2, df_test_v2),\n",
        "    'v3': (df_train_v3, df_test_v3)\n",
        "}\n",
        "\n",
        "embeddings_ft = {\n",
        "    'BOW_default': \"TfidfVectorizer()\"\n",
        "}\n",
        "\n",
        "models_ft = {\n",
        "    'lr-elasticnet0.5': \"LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga', max_iter=1000, random_state=rs)\",\n",
        "    'lr-elasticnet0.25': \"LogisticRegression(penalty='elasticnet', l1_ratio=0.25, solver='saga', max_iter=1000, random_state=rs)\",\n",
        "    'lr-elasticnet0.75': \"LogisticRegression(penalty='elasticnet', l1_ratio=0.75, solver='saga', max_iter=1000, random_state=rs)\",\n",
        "    'svm-10': \"svm.LinearSVC(C=10, random_state=rs)\",\n",
        "    'svm-2': \"svm.LinearSVC(C=2, random_state=rs)\",\n",
        "    'svm-0.5': \"svm.LinearSVC(C=0.5, random_state=rs)\",\n",
        "    'svm-0.05': \"svm.LinearSVC(C=0.05, random_state=rs)\",\n",
        "}\n",
        "\n",
        "save_to = 'drive/MyDrive/ML_models/'\n",
        "\n",
        "for i in X_ft:\n",
        "    for j in embeddings_ft:\n",
        "        for k in models_ft:\n",
        "            m_v = my_ML(i, j, k, X_ft, embeddings_ft, models_ft)\n",
        "            with open(f'{save_to}{i}-{j}-{k}.pickle', 'wb') as f:\n",
        "                pickle.dump(m_v, f)\n",
        "\n",
        "my_df_to_gDrive(df_results.loc[df_results.index.str.startswith('ML - ')], \"df_final_ML_results.csv\", folder_id, True)"
      ],
      "metadata": {
        "id": "Ybd3lsqn_6xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_gDrive_to_colab('df_final_ML_results.csv', folder_id)\n",
        "\n",
        "df_ML_results = pd.read_csv('df_final_ML_results.csv')"
      ],
      "metadata": {
        "id": "4IduL99-MzTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujzrSQpIWrqz"
      },
      "source": [
        "# Using Transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_load_model_from_name(model_name):\n",
        "    tk = AutoTokenizer.from_pretrained(model_name)\n",
        "    try:\n",
        "        md = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        md = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
        "    md.compile(optimizer=Adam(3e-5))\n",
        "    return (tk, md)\n",
        "\n",
        "def my_fine_tune(texts, labels, md, tk):\n",
        "    tk_data = dict(tk(texts, return_tensors=\"np\", padding=True))\n",
        "    md.fit(tk_data, labels)\n",
        "    return md\n",
        "\n",
        "def my_fine_tune_pipeline(df, model_name, batch_size=16, epochs=2):\n",
        "    dfs = [df.iloc[i:i+batch_size].copy() for i in range(0, len(df), batch_size)]\n",
        "    tk, md = my_load_model_from_name(model_name)\n",
        "    for epoch in range(epochs):\n",
        "        for dff in dfs:\n",
        "            md = my_fine_tune(list(dff['text']), np.where(dff['label']=='pos', 1, 0), md, tk)\n",
        "    return (tk, md)\n",
        "\n",
        "# Aplicação dos transformadores no conjunto de teste (demora cerca de 3 horas por transformador)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "df_transformers = df_test.copy() # .sample(frac=0.5, random_state=rs).copy()\n",
        "\n",
        "models = [\n",
        "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    \"lvwerra/distilbert-imdb\",\n",
        "]\n",
        "\n",
        "df_transformers['y_true'] = np.where(df_transformers['label'] == 'pos', 1, 0)\n",
        "transformers_y_true = df_transformers['y_true']\n",
        "transformers_cm = []\n",
        "\n",
        "df_transformers['y_true'] = np.where(df_transformers['label'] == 'pos', 1, 0)\n",
        "transformers_y_true = df_transformers['y_true']\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=models[0]) #, top_k=2)\n",
        "df_transformers[f'pred_label_{models[0][:3]}'] = df_transformers['text'].map(lambda x: classifier(x))\n",
        "\n",
        "\n",
        "my_df_to_gDrive(\n",
        "    df_transformers,\n",
        "    f\"df_transformers_{str(datetime.datetime.now())[:19].replace(' ', '_').replace(':', '')}.csv\",\n",
        "    folder_id,\n",
        "    True\n",
        ")\n",
        "\n",
        "df_transformers['y_true'] = np.where(df_transformers['label'] == 'pos', 1, 0)\n",
        "transformers_y_true = df_transformers['y_true']\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=models[1]) #, top_k=2)\n",
        "df_transformers[f'pred_label_{models[1][:3]}'] = df_transformers['text'].map(lambda x: classifier(x))\n",
        "\n",
        "my_df_to_gDrive(\n",
        "    df_transformers,\n",
        "    f\"df_transformers_{str(datetime.datetime.now())[:19].replace(' ', '_').replace(':', '')}.csv\",\n",
        "    folder_id,\n",
        "    True\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Fine tunning dos transformadores\n",
        "\n",
        "\"\"\"\n",
        "model_name = \"lvwerra/distilbert-imdb\"\n",
        "\n",
        "df_shuffled = df_train.copy().sample(frac=1, random_state=rs)\n",
        "batch_size = 5000\n",
        "\n",
        "for dff_shuffled in [df_shuffled.iloc[i:i+batch_size].copy() for i in range(0, len(df_shuffled), batch_size)]:\n",
        "    ft_name = model_name\n",
        "\n",
        "    tk, md = my_fine_tune_pipeline(dff_shuffled, ft_name, 20, 1)\n",
        "\n",
        "    curr_timestamp = str(datetime.datetime.now())[:19].replace(' ', '_').replace(':', '')\n",
        "    ft_name = f\"drive/MyDrive/TMCD/my_fine_tune_5000_{model_name.replace('/', '-')}_{curr_timestamp}\"\n",
        "\n",
        "    md.save_pretrained(ft_name)\n",
        "    tk.save_pretrained(ft_name)\n",
        "\n",
        "    dff = df_test.copy().sample(n=20)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=ft_name) #, top_k=2)\n",
        "    dff[f'mft_{curr_timestamp}'] = dff['text'].map(lambda x: classifier(x))\n",
        "\n",
        "    display(dff)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "model_name = 'drive/MyDrive/TMCD/my_fine_tune_5000_lvwerra-distilbert-imdb_2023-03-24_174929'\n",
        "model_name_base = 'lvwerra-distilbert-imdb'\n",
        "\n",
        "df_shuffled = df_train.copy().sample(frac=1, random_state=rs)\n",
        "\n",
        "ft_name = model_name\n",
        "\n",
        "for i in range(6):\n",
        "\n",
        "    tk, md = my_fine_tune_pipeline(df_shuffled, ft_name, 20, 1)\n",
        "\n",
        "    curr_timestamp = str(datetime.datetime.now())[:19].replace(' ', '_').replace(':', '')\n",
        "    ft_name = f\"drive/MyDrive/TMCD/my_fine_tune_5000_{i}_{model_name_base.replace('/', '-')}_{curr_timestamp}\"\n",
        "\n",
        "    md.save_pretrained(ft_name)\n",
        "    tk.save_pretrained(ft_name)\n",
        "\n",
        "    dff = df_test.copy().sample(n=20)\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=ft_name) #, top_k=2)\n",
        "    dff[f'mft_{curr_timestamp}'] = dff['text'].map(lambda x: classifier(x))\n",
        "\n",
        "    display(dff)\n",
        "\"\"\"\n",
        "\n",
        "# Aplicação dos transformadores com fine tuning no conjunto de teste (demora cerca de 3 horas por transformador)\n",
        "\n",
        "\"\"\"\n",
        "my_fine_tuned_models = [\n",
        "    'my_fine_tune_3_lvwerra-distilbert-imdb',\n",
        "    'my_fine_tune_1_epoch_lvwerra-distilbert-imdb',\n",
        "    'my_fine_tune_5_lvwerra-distilbert-imdb',\n",
        "    'my_fine_tune_2_epoch_lvwerra-distilbert-imdb',\n",
        "    'my_fine_tune_4_lvwerra-distilbert-imdb'\n",
        "]\n",
        "\n",
        "name_order = [3,1,5,2,4]\n",
        "\n",
        "for i in my_fine_tuned_models:\n",
        "    with zipfile.ZipFile(i+'.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "df_transformers = df_test.copy() # .sample(frac=0.5, random_state=rs).copy()\n",
        "\n",
        "\n",
        "for j, mftm in enumerate(my_fine_tuned_models):\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=mftm) #, top_k=2)\n",
        "    df_transformers[f'pred_label_mft_{name_order[j]}'] = df_transformers['text'].map(lambda x: classifier(x))\n",
        "\n",
        "    tstamp = str(datetime.datetime.now())[:19].replace(' ', '_').replace(':', '')\n",
        "    df_transformers.to_csv(f\"df_mft_{tstamp}.csv\", index=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jed8mJu9B9jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z85JVf5IWvU2"
      },
      "outputs": [],
      "source": [
        "my_gDrive_to_colab('df_transformers_2023-03-21_190316.csv', folder_id)\n",
        "\n",
        "df_transformers = pd.read_csv('df_transformers_2023-03-21_190316.csv', index_col=0)\n",
        "\n",
        "df_transformers['y_dis'] = df_transformers['pred_label_dis'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "df_transformers['y_lvw'] = df_transformers['pred_label_lvw'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_transformers['y_true'], 'y_pred': df_transformers['y_dis']}),\n",
        "    df_results,\n",
        "    \"Transformers - distilbert-sst2\"\n",
        ")\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_transformers['y_true'], 'y_pred': df_transformers['y_lvw']}),\n",
        "    df_results,\n",
        "    \"Transformers - distilbert-imdb\"\n",
        ")\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_gDrive_to_colab('df_mft_2023-03-25_140738.csv', folder_id)\n",
        "\n",
        "df_mft = pd.read_csv('df_mft_2023-03-25_140738.csv', index_col=0)\n",
        "\n",
        "df_mft['y_true'] = df_mft['label'].map(lambda x: 1 if x == 'pos' else 0)\n",
        "\n",
        "df_mft['y_mft_1'] = df_mft['pred_label_mft_1'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "df_mft['y_mft_2'] = df_mft['pred_label_mft_2'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "df_mft['y_mft_3'] = df_mft['pred_label_mft_3'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "df_mft['y_mft_4'] = df_mft['pred_label_mft_4'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "df_mft['y_mft_5'] = df_mft['pred_label_mft_5'].map(lambda x: 1 if eval(x)[0]['label'] == 'POSITIVE' else 0)\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_mft['y_true'], 'y_pred': df_mft['y_mft_1']}),\n",
        "    df_results,\n",
        "    \"Transformers - Fine Tuned 1 Epoch\"\n",
        ")\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_mft['y_true'], 'y_pred': df_mft['y_mft_2']}),\n",
        "    df_results,\n",
        "    \"Transformers - Fine Tuned 2 Epoch\"\n",
        ")\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_mft['y_true'], 'y_pred': df_mft['y_mft_3']}),\n",
        "    df_results,\n",
        "    \"Transformers - Fine Tuned 3 Epoch\"\n",
        ")\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_mft['y_true'], 'y_pred': df_mft['y_mft_4']}),\n",
        "    df_results,\n",
        "    \"Transformers - Fine Tuned 4 Epoch\"\n",
        ")\n",
        "\n",
        "my_register_scores(\n",
        "    pd.DataFrame({'y_true': df_mft['y_true'], 'y_pred': df_mft['y_mft_5']}),\n",
        "    df_results,\n",
        "    \"Transformers - Fine Tuned 5 Epoch\"\n",
        ")\n",
        "\n",
        "\n",
        "df_results"
      ],
      "metadata": {
        "id": "Nr9F9vCtpQxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRMqhRIeWvrl"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXu4ix0-WzPi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_name = 'df_final_ML_results.csv'\n",
        "my_gDrive_to_colab(df_name, folder_id)\n",
        "df_ML_results = pd.read_csv(df_name, index_col=0)\n",
        "ini_index = list(df_ML_results.index)\n",
        "for i in ini_index:\n",
        "    if i.endswith('train'):\n",
        "        df_ML_results.loc[i.replace(' - train', ''), 'accuracy_train'] = df_ML_results.loc[i, 'accuracy_score']\n",
        "        df_ML_results.loc[i.replace(' - train', ''), 'f1_train'] = df_ML_results.loc[i, 'f1_score']\n",
        "    elif i.endswith('test'):\n",
        "        df_ML_results.loc[i.replace(' - test', ''), 'accuracy_test'] = df_ML_results.loc[i, 'accuracy_score']\n",
        "        df_ML_results.loc[i.replace(' - test', ''), 'f1_test'] = df_ML_results.loc[i, 'f1_score']\n",
        "df_ML_results = df_ML_results.loc[~df_ML_results.index.isin(ini_index), ['accuracy_train', 'f1_train', 'accuracy_test', 'f1_test']].copy()\n",
        "df_ML_results['model'] = df_ML_results.index\n",
        "df_ML_results[['preprocessing', 'embedding', 'model']] = df_ML_results['model'].str.replace('ML - ', '').str.split(' - ', expand=True)\n",
        "\n",
        "df_ML_results_preprocessing = df_ML_results.sort_values('accuracy_test', ascending=False).groupby('preprocessing').first()[['accuracy_train', 'accuracy_test', 'embedding' ,'model']].sort_values('accuracy_test', ascending=False)\n",
        "df_ML_results_embedding = df_ML_results.sort_values('accuracy_test', ascending=False).groupby('embedding').first()[['accuracy_train', 'accuracy_test', 'preprocessing', 'model']].sort_values('accuracy_test', ascending=False)\n",
        "df_ML_results_model = df_ML_results.sort_values('accuracy_test', ascending=False).groupby('model').first()[['accuracy_train', 'accuracy_test', 'preprocessing', 'embedding']].sort_values('accuracy_test', ascending=False)\n",
        "\n",
        "display(df_ML_results_preprocessing)\n",
        "display(df_ML_results_embedding)\n",
        "display(df_ML_results_model)\n",
        "\n",
        "plt.figure(figsize=(21,5))\n",
        "model_names = df_ML_results_model.index + '\\n(' + df_ML_results_model['preprocessing'] + ')\\n(' + df_ML_results_model['embedding'] + ')'\n",
        "\n",
        "for i, j in enumerate(zip(df_ML_results_model['accuracy_train'], df_ML_results_model['accuracy_test'])):\n",
        "    plt.plot([i, i], [j[0], j[1]], '-', color='red', alpha=0.5, linewidth=2)\n",
        "\n",
        "plt.plot(model_names, df_ML_results_model['accuracy_train'], 'o', color='blue', label='Train Accuracy')\n",
        "plt.plot(model_names, df_ML_results_model['accuracy_test'], 'o', color='green', label='Test Accuracy')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Test Accuracy for Different Models (w/ Best Preprocessing and Embedding)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# good_models = [\n",
        "#     'drive/MyDrive/ML_models/v3-BOW_default-lr-elasticnet0.75.pickle',\n",
        "#     'drive/MyDrive/ML_models/v3-BOW_default-lr-elasticnet0.25.pickle',\n",
        "#     'drive/MyDrive/ML_models/v3-BOW_default-lr.pickle'\n",
        "# ]\n",
        "# \n",
        "# with open(good_models[1], 'rb') as f:\n",
        "#     clf, vec = pickle.load(f)\n",
        "# \n",
        "# string = df_train.iloc[0,0] \n",
        "# p, s, v = my_sa(string, prep_v3, vec, clf)\n",
        "# my_wc(v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ML_results_model_lr_svm = df_ML_results_model.loc[['lr', 'lr-elasticnet0.25', 'lr-elasticnet0.5', 'lr-elasticnet0.75', 'svm-0.05', 'svm-0.5', 'svm', 'svm-2', 'svm-10'], :]\n",
        "\n",
        "plt.figure(figsize=(13,4))\n",
        "model_names = df_ML_results_model_lr_svm.index + '\\n(' + df_ML_results_model_lr_svm['preprocessing'] + ')\\n(' + df_ML_results_model_lr_svm['embedding'] + ')'\n",
        "\n",
        "for i, j in enumerate(zip(df_ML_results_model_lr_svm['accuracy_train'], df_ML_results_model_lr_svm['accuracy_test'])):\n",
        "    plt.plot([i, i], [j[0], j[1]], '-', color='red', alpha=0.5, linewidth=2)\n",
        "\n",
        "plt.plot(model_names, df_ML_results_model_lr_svm['accuracy_train'], 'o', color='blue', label='Train Accuracy')\n",
        "plt.plot(model_names, df_ML_results_model_lr_svm['accuracy_test'], 'o', color='green', label='Test Accuracy')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Test Accuracy for Different Models (w/ Best Preprocessing and Embedding)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Fw2b1pAywerG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_ft = df_results[df_results.index.map(lambda x: 'Fine Tuned' in x)]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "\n",
        "plt.plot([\"1\", \"2\", \"3\", \"4\", \"5\"], df_results_ft['accuracy_score'], '-o')\n",
        "\n",
        "plt.xlabel('# of Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Over Epochs in Fine Tuning')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "fxrTWB5cXYYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}